{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrap import Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://medium.com/@bijit211987/multimodal-retrieval-augmented-generation-mm-rag-2e8f6dc59f11\"\n",
    "scrp = Scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrp.scrape_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAG_QnA import RAG_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 15:45:15.983369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-28 15:45:16.013969: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-28 15:45:16.020627: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-28 15:45:17.696623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/junaid-ul-hassan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "rag_model = RAG_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 50 is greater than number of elements in index 32, updating n_results = 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't have information about a specific system called NUML. However, from the context provided, it seems like NUML could refer to a purpose-built vector database used in multimodal AI applications. These types of databases help scale and deploy AI systems by enabling efficient storage and retrieval of multimedia data, such as images, audio, video, and text, in a common semantic embedding space. They allow for any-to-any search, where a query can be matched with relevant results, regardless of the original modality of the query or the results. Thanks for asking!</s>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_model.generateResponse(prompt=\"What is NUML?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='audio, and text examples to inform its completions. This grounds the output in additional context, improving relevance and accuracy. Early\\nresults from Anthropic using this approach are incredible. On difficult prompts like imagining a day at the beach, models augmented\\nwith image retrieval produce vastly more coherent, grounded, and specific outputs: Generated without MM-RAG : I imagine a day at\\nthe beach would be very relaxing. I would sit in the sun and listen to the waves crash along the\\nshore. Maybe I would go for a swim or build a sandcastle. It would be nice to get away from\\nnormal life for a while and enjoy the peaceful atmosphere. Generated with MM-RAG: I imagine a day at the beach\\nfilled with golden sandy shores and the rhythmic crash of bright blue waves lapping gently at the coastline. My toes\\nwould sink into smooth sand as I breathed in the fresh and briny sea air under a bright sky, dotted', metadata={'source': '/media/junaid-ul-hassan/248ac48e-ccd4-4707-a28b-33cb7a46e6dc/LLMs Projects/Web_pilot/text_file.txt/text_file.txt'}),\n",
       " Document(page_content='model retrieves relevant image, audio, and text examples to inform its completions. This grounds the output in additional context, improving\\nrelevance and accuracy. Early results from Anthropic using this approach are incredible. On difficult prompts like imagining a day at\\nthe beach, models augmented with image retrieval produce vastly more coherent, grounded, and specific outputs: Generated without MM-RAG : I\\nimagine a day at the beach would be very relaxing. I would sit in the sun and listen to the\\nwaves crash along the shore. Maybe I would go for a swim or build a sandcastle. It would be nice\\nto get away from normal life for a while and enjoy the peaceful atmosphere. Generated with MM-RAG: I imagine a\\nday at the beach filled with golden sandy shores and the rhythmic crash of bright blue waves lapping gently at\\nthe coastline. My toes would sink into smooth sand as I breathed in the fresh and briny sea air under', metadata={'source': '/media/junaid-ul-hassan/248ac48e-ccd4-4707-a28b-33cb7a46e6dc/LLMs Projects/Web_pilot/text_file.txt/text_file.txt'}),\n",
       " Document(page_content='incredible capabilities discussed in production environments. I hope you’ve enjoyed this tour through the growing world of multimodal intelligence and\\nlearning how databases are powering real-world applications leveraging techniques from contrastive learning to MM-RAG! Exciting times are ahead as multimodal\\nand generative AI continue rapid development. Multimodal Retrieval Augmented Generation (MM-RAG) Bijit Ghosh · Follow 6 min read · Jan\\n21, 2024 -- Listen Share Introduction Multimodal machine learning is revolutionizing what AI systems can do. By understanding different modalities\\nlike images, audio, video, and text, these systems can solve problems that were previously intractable for machines. Let’s explore an\\nexciting development in this field — Multimodal Retrieval Augmented Generation (MM-RAG) — and how vector databases enable us to build', metadata={'source': '/media/junaid-ul-hassan/248ac48e-ccd4-4707-a28b-33cb7a46e6dc/LLMs Projects/Web_pilot/text_file.txt/text_file.txt'}),\n",
       " Document(page_content='Multimodal Retrieval Augmented Generation (MM-RAG) Introduction Contrastive Learning for Multimodal Representations Any-to-Any Search with Multimodal Embeddings MM-RAG: Augmenting Generation with\\nMultimodal Retrieval Building Multimodal Production Systems with Vector Databases The Future of Multimodal AI Sign up Sign in Sign up\\nSign in Bijit Ghosh Follow -- Listen Share Multimodal machine learning is revolutionizing what AI systems can do. By understanding\\ndifferent modalities like images, audio, video, and text, these systems can solve problems that were previously intractable for machines. Let’s\\nexplore an exciting development in this field — Multimodal Retrieval Augmented Generation (MM-RAG) — and how vector databases enable us\\nto build practical applications powered by multimodal embeddings for any-to-any search and retrieval. We’ll start by explaining contrastive learning, a', metadata={'source': '/media/junaid-ul-hassan/248ac48e-ccd4-4707-a28b-33cb7a46e6dc/LLMs Projects/Web_pilot/text_file.txt/text_file.txt'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"What is RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
