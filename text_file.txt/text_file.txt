Multimodal Retrieval Augmented Generation (MM-RAG) Introduction Contrastive Learning for Multimodal Representations Any-to-Any Search with Multimodal Embeddings MM-RAG: Augmenting Generation with
Multimodal Retrieval Building Multimodal Production Systems with Vector Databases The Future of Multimodal AI Sign up Sign in Sign up
Sign in Bijit Ghosh Follow -- Listen Share Multimodal machine learning is revolutionizing what AI systems can do. By understanding
different modalities like images, audio, video, and text, these systems can solve problems that were previously intractable for machines. Let’s
explore an exciting development in this field — Multimodal Retrieval Augmented Generation (MM-RAG) — and how vector databases enable us
to build practical applications powered by multimodal embeddings for any-to-any search and retrieval. We’ll start by explaining contrastive learning, a
technique for training high-quality multimodal embeddings. We’ll then discuss how these embeddings empower any-to-any search across modalities. Next, we’ll dive
into MM-RAG and see how retrieving relevant multimodal context can dramatically improve text generation. Finally, we’ll look at leveraging vector
databases to make deployment of these techniques possible at scale. Let’s get started! Contrastive learning has emerged as a powerful
approach for training multimodal machine learning models. The key insight is that we can leverage contrastive examples — similar and
dissimilar data points across modalities — to teach models useful multimodal representations. For instance, we can train an image-text model
by showing it matching image-caption pairs as positive examples, and mismatched pairs as negative examples. By pushing embeddings for the
positive pairs close together while separating embeddings for negative pairs, the model learns alignments between textual and visual concepts. Techniques
like CLIP (Contrastive Language-Image Pre-training) have used this approach to great effect. The authors trained an image-text model on 400
million image-caption pairs harvested from the internet, yielding an extremely capable multimodal embedding model. The same principles apply for other
modality combinations as well. For example, speech-to-text models can be trained with matching audio waveform-transcript pairs as positive examples. Video-text
models can use paired video clips and descriptions. Contrastive learning even applies to uni-modal scenarios like learning text-to-text representations. The
key ingredients for effective contrastive learning are: When done right, contrastive learning yields encodings that cluster semantic concepts across modalities.
This enables powerful cross-modal search and retrieval. Multimodal machine learning unlocks an exciting capability — any-to-any search across modalities. With
high-quality joint embeddings, we can find related content regardless of format — retrieving images that match text queries, finding audio
clips that overlap the same concepts as video, etc. This works by encoding all of our data points — images,
audio, video, text documents — into the same semantic embedding space. Items with closer embeddings capture similar concepts, while those
farther apart are more distinct. To enable any-to-any search, we simply encode our query into the shared space, then retrieve
the nearest neighbors across our entire database based on embedding similarity. This surfaced relevant matches regardless of whether the query
and results are images, text, audio, or video! For example, let’s walk through an image-to-text search scenario: The same approach
applies for any other modality for queries and searched database. We can go from text-to-audio, video-to-images, audio-to-video, and every other
combination. Contrastive learning representations make this possible — grouping semantic concepts close together regardless of modality. This paves the way
for incredibly flexible, powerful multimodal search and discovery! Multimodal embeddings have enabled models that understand the relationships between images, text,
audio, video and more. Researchers are now exploring how to apply this advanced representation learning to improve language generation models.
One technique gaining traction is Multimodal Retrieval Augmented Generation (MM-RAG). In a nutshell, the key idea is to combine a
language model like GPT-3 with a multimodal retriever using contrastive learning embeddings. During text generation, the model retrieves relevant image,
audio, and text examples to inform its completions. This grounds the output in additional context, improving relevance and accuracy. Early
results from Anthropic using this approach are incredible. On difficult prompts like imagining a day at the beach, models augmented
with image retrieval produce vastly more coherent, grounded, and specific outputs: Generated without MM-RAG : I imagine a day at
the beach would be very relaxing. I would sit in the sun and listen to the waves crash along the
shore. Maybe I would go for a swim or build a sandcastle. It would be nice to get away from
normal life for a while and enjoy the peaceful atmosphere. Generated with MM-RAG: I imagine a day at the beach
filled with golden sandy shores and the rhythmic crash of bright blue waves lapping gently at the coastline. My toes
would sink into smooth sand as I breathed in the fresh and briny sea air under a bright sky, dotted
with puffy clouds. I’d love to go snorkeling and glimpse the colorful fish swimming below or just nap on a
towel, lulled into a rest by the glittering water and crying seagulls swooping overhead. Maybe later I could crack open
a coconut or build an elaborate sandcastle with bridges and moats before taking a long walk at sunset, watching the
glowing orange disk sink below the horizon. The MM-RAG example conveys so much more specificity and flair thanks to grounding
from relevant imagery. Researchers have found similar benefits across many types of generative tasks from summarization to dialogue modeling and
more. This approach holds incredible promise for taking language AI to the next level. But we need scalable ways to
deliver both multimodal retrieval and foundation models in production systems. Next let’s explore how vector databases make this possible. The
techniques we’ve explored offer game-changing capabilities — multimodal search and retrieval, MM-RAG grounded language generation, and more. But delivering these
systems at scale in production comes with challenges around efficiency, cost, infrastructure complexity and more. This is where purpose-built vector
databases like Weaviate come in. By efficiently indexing and searching vector spaces, these databases unlock user-scale deployment of contrastive learning
and neural representation models. For example, to enable super-fast multimodal search and retrieval, we can use the following stack: With
specialized data structures and algorithms tailored to vector spaces, this stack powers blazing fast multi-modal search even with huge data
volumes. We can use the same foundation to build MM-RAG production systems at scale: By leveraging the combined powers of
foundation models, contrastive representation learning, and vector databases — this systems moves the promises of MM-RAG from research into real-world
practice. This post has highlighted fascinating innovations like contrastive learning for multimodal representations, any-to-any search across modalities, and MM-RAG grounded
generation. Together, these techniques are expanding what AI can perceive, conceive and achieve. As methods mature further, we’ll see multimodal
AI grow more pervasive — from recommender systems like Meta understanding users rich interests, to virtual assistants like GPT-4 and
others answering questions more accurately. Media & e-commerce will allow fine-grained exploration of catalogs with any-to-any search. Generative applications will
produce writing, imagery, animation, synthesis and dialogue that maintains strong coherence and grounding. Making these futures reality requires that we
make multimodal AI scalable and deployable. Purpose-built vector databases unlock this critical step, enabling the incredible capabilities discussed in production
environments. I hope you’ve enjoyed this tour through the growing world of multimodal intelligence and learning how databases are powering
real-world applications leveraging techniques from contrastive learning to MM-RAG! Exciting times are ahead as multimodal and generative AI continue rapid
development. -- -- CTO | Senior Engineering Leader focused on Cloud Native | AI/ML | DevSecOps Help Status About Careers
Press Blog Privacy Terms Text to speech Teams Multimodal Retrieval Augmented Generation (MM-RAG) Bijit Ghosh · Follow 6 min read
· Jan 21, 2024 -- Listen Share Introduction Multimodal machine learning is revolutionizing what AI systems can do. By understanding
different modalities like images, audio, video, and text, these systems can solve problems that were previously intractable for machines. Let’s
explore an exciting development in this field — Multimodal Retrieval Augmented Generation (MM-RAG) — and how vector databases enable us
to build practical applications powered by multimodal embeddings for any-to-any search and retrieval. We’ll start by explaining contrastive learning, a
technique for training high-quality multimodal embeddings. We’ll then discuss how these embeddings empower any-to-any search across modalities. Next, we’ll dive
into MM-RAG and see how retrieving relevant multimodal context can dramatically improve text generation. Finally, we’ll look at leveraging vector
databases to make deployment of these techniques possible at scale. Let’s get started! Contrastive Learning for Multimodal Representations Contrastive learning
has emerged as a powerful approach for training multimodal machine learning models. The key insight is that we can leverage
contrastive examples — similar and dissimilar data points across modalities — to teach models useful multimodal representations. For instance, we
can train an image-text model by showing it matching image-caption pairs as positive examples, and mismatched pairs as negative examples.
By pushing embeddings for the positive pairs close together while separating embeddings for negative pairs, the model learns alignments between
textual and visual concepts. Techniques like CLIP (Contrastive Language-Image Pre-training) have used this approach to great effect. The authors trained
an image-text model on 400 million image-caption pairs harvested from the internet, yielding an extremely capable multimodal embedding model. The
same principles apply for other modality combinations as well. For example, speech-to-text models can be trained with matching audio waveform-transcript
pairs as positive examples. Video-text models can use paired video clips and descriptions. Contrastive learning even applies to uni-modal scenarios
like learning text-to-text representations. The key ingredients for effective contrastive learning are: Lots of paired examples for aligning concepts across
modalities An appropriate contrastive loss function to compare positive and negative examples Sufficient model capacity to learn high-quality joint representations
When done right, contrastive learning yields encodings that cluster semantic concepts across modalities. This enables powerful cross-modal search and retrieval.
Any-to-Any Search with Multimodal Embeddings Multimodal machine learning unlocks an exciting capability — any-to-any search across modalities. With high-quality joint
embeddings, we can find related content regardless of format — retrieving images that match text queries, finding audio clips that
overlap the same concepts as video, etc. This works by encoding all of our data points — images, audio, video,
text documents — into the same semantic embedding space. Items with closer embeddings capture similar concepts, while those farther apart
are more distinct. To enable any-to-any search, we simply encode our query into the shared space, then retrieve the nearest
neighbors across our entire database based on embedding similarity. This surfaced relevant matches regardless of whether the query and results
are images, text, audio, or video! For example, let’s walk through an image-to-text search scenario: Encode image database into multimodal
embedding space User provides query image of a beach Encode query image into same embedding space Retrieve text documents with
closest embeddings to query image Return relevant results about beaches, oceans, sand, waves, sunsets, etc. The same approach applies for
any other modality for queries and searched database. We can go from text-to-audio, video-to-images, audio-to-video, and every other combination. Contrastive
learning representations make this possible — grouping semantic concepts close together regardless of modality. This paves the way for incredibly
flexible, powerful multimodal search and discovery! MM-RAG: Augmenting Generation with Multimodal Retrieval Multimodal embeddings have enabled models that understand the
relationships between images, text, audio, video and more. Researchers are now exploring how to apply this advanced representation learning to
improve language generation models. One technique gaining traction is Multimodal Retrieval Augmented Generation (MM-RAG). In a nutshell, the key idea
is to combine a language model like GPT-3 with a multimodal retriever using contrastive learning embeddings. During text generation, the
model retrieves relevant image, audio, and text examples to inform its completions. This grounds the output in additional context, improving
relevance and accuracy. Early results from Anthropic using this approach are incredible. On difficult prompts like imagining a day at
the beach, models augmented with image retrieval produce vastly more coherent, grounded, and specific outputs: Generated without MM-RAG : I
imagine a day at the beach would be very relaxing. I would sit in the sun and listen to the
waves crash along the shore. Maybe I would go for a swim or build a sandcastle. It would be nice
to get away from normal life for a while and enjoy the peaceful atmosphere. Generated with MM-RAG: I imagine a
day at the beach filled with golden sandy shores and the rhythmic crash of bright blue waves lapping gently at
the coastline. My toes would sink into smooth sand as I breathed in the fresh and briny sea air under
a bright sky, dotted with puffy clouds. I’d love to go snorkeling and glimpse the colorful fish swimming below or
just nap on a towel, lulled into a rest by the glittering water and crying seagulls swooping overhead. Maybe later
I could crack open a coconut or build an elaborate sandcastle with bridges and moats before taking a long walk
at sunset, watching the glowing orange disk sink below the horizon. The MM-RAG example conveys so much more specificity and
flair thanks to grounding from relevant imagery. Researchers have found similar benefits across many types of generative tasks from summarization
to dialogue modeling and more. This approach holds incredible promise for taking language AI to the next level. But we
need scalable ways to deliver both multimodal retrieval and foundation models in production systems. Next let’s explore how vector databases
make this possible. Building Multimodal Production Systems with Vector Databases The techniques we’ve explored offer game-changing capabilities — multimodal search
and retrieval, MM-RAG grounded language generation, and more. But delivering these systems at scale in production comes with challenges around
efficiency, cost, infrastructure complexity and more. This is where purpose-built vector databases like Weaviate come in. By efficiently indexing and
searching vector spaces, these databases unlock user-scale deployment of contrastive learning and neural representation models. For example, to enable super-fast
multimodal search and retrieval, we can use the following stack: Contrastively trained multimodal models like CLIP encode data points like
images, text, audio etc. into a shared vector space Vector database like Weaviate ingests these embeddings Database performs efficient vector
similarity search even with billions of embeddings User queries search vector database to instantly retrieve relevant results across modalities With
specialized data structures and algorithms tailored to vector spaces, this stack powers blazing fast multi-modal search even with huge data
volumes. We can use the same foundation to build MM-RAG production systems at scale: Ingest large foundation language model Allow
model to interface with vector database storing billions of multimodal embeddings During generation, efficiently retrieve relevant context to inform text
completions Return grounded, relevant output to users Continue to improve models through feedback loops By leveraging the combined powers of
foundation models, contrastive representation learning, and vector databases — this systems moves the promises of MM-RAG from research into real-world
practice. The Future of Multimodal AI This post has highlighted fascinating innovations like contrastive learning for multimodal representations, any-to-any search
across modalities, and MM-RAG grounded generation. Together, these techniques are expanding what AI can perceive, conceive and achieve. As methods
mature further, we’ll see multimodal AI grow more pervasive — from recommender systems like Meta understanding users rich interests, to
virtual assistants like GPT-4 and others answering questions more accurately. Media & e-commerce will allow fine-grained exploration of catalogs with
any-to-any search. Generative applications will produce writing, imagery, animation, synthesis and dialogue that maintains strong coherence and grounding. Making these
futures reality requires that we make multimodal AI scalable and deployable. Purpose-built vector databases unlock this critical step, enabling the
incredible capabilities discussed in production environments. I hope you’ve enjoyed this tour through the growing world of multimodal intelligence and
learning how databases are powering real-world applications leveraging techniques from contrastive learning to MM-RAG! Exciting times are ahead as multimodal
and generative AI continue rapid development. Multimodal Retrieval Augmented Generation (MM-RAG) Bijit Ghosh · Follow 6 min read · Jan
21, 2024 -- Listen Share Introduction Multimodal machine learning is revolutionizing what AI systems can do. By understanding different modalities
like images, audio, video, and text, these systems can solve problems that were previously intractable for machines. Let’s explore an
exciting development in this field — Multimodal Retrieval Augmented Generation (MM-RAG) — and how vector databases enable us to build
practical applications powered by multimodal embeddings for any-to-any search and retrieval. We’ll start by explaining contrastive learning, a technique for
training high-quality multimodal embeddings. We’ll then discuss how these embeddings empower any-to-any search across modalities. Next, we’ll dive into MM-RAG
and see how retrieving relevant multimodal context can dramatically improve text generation. Finally, we’ll look at leveraging vector databases to
make deployment of these techniques possible at scale. Let’s get started! Contrastive Learning for Multimodal Representations Contrastive learning has emerged
as a powerful approach for training multimodal machine learning models. The key insight is that we can leverage contrastive examples
— similar and dissimilar data points across modalities — to teach models useful multimodal representations. For instance, we can train
an image-text model by showing it matching image-caption pairs as positive examples, and mismatched pairs as negative examples. By pushing
embeddings for the positive pairs close together while separating embeddings for negative pairs, the model learns alignments between textual and
visual concepts. Techniques like CLIP (Contrastive Language-Image Pre-training) have used this approach to great effect. The authors trained an image-text
model on 400 million image-caption pairs harvested from the internet, yielding an extremely capable multimodal embedding model. The same principles
apply for other modality combinations as well. For example, speech-to-text models can be trained with matching audio waveform-transcript pairs as
positive examples. Video-text models can use paired video clips and descriptions. Contrastive learning even applies to uni-modal scenarios like learning
text-to-text representations. The key ingredients for effective contrastive learning are: Lots of paired examples for aligning concepts across modalities An
appropriate contrastive loss function to compare positive and negative examples Sufficient model capacity to learn high-quality joint representations When done
right, contrastive learning yields encodings that cluster semantic concepts across modalities. This enables powerful cross-modal search and retrieval. Any-to-Any Search
with Multimodal Embeddings Multimodal machine learning unlocks an exciting capability — any-to-any search across modalities. With high-quality joint embeddings, we
can find related content regardless of format — retrieving images that match text queries, finding audio clips that overlap the
same concepts as video, etc. This works by encoding all of our data points — images, audio, video, text documents
— into the same semantic embedding space. Items with closer embeddings capture similar concepts, while those farther apart are more
distinct. To enable any-to-any search, we simply encode our query into the shared space, then retrieve the nearest neighbors across
our entire database based on embedding similarity. This surfaced relevant matches regardless of whether the query and results are images,
text, audio, or video! For example, let’s walk through an image-to-text search scenario: Encode image database into multimodal embedding space
User provides query image of a beach Encode query image into same embedding space Retrieve text documents with closest embeddings
to query image Return relevant results about beaches, oceans, sand, waves, sunsets, etc. The same approach applies for any other
modality for queries and searched database. We can go from text-to-audio, video-to-images, audio-to-video, and every other combination. Contrastive learning representations
make this possible — grouping semantic concepts close together regardless of modality. This paves the way for incredibly flexible, powerful
multimodal search and discovery! MM-RAG: Augmenting Generation with Multimodal Retrieval Multimodal embeddings have enabled models that understand the relationships between
images, text, audio, video and more. Researchers are now exploring how to apply this advanced representation learning to improve language
generation models. One technique gaining traction is Multimodal Retrieval Augmented Generation (MM-RAG). In a nutshell, the key idea is to
combine a language model like GPT-3 with a multimodal retriever using contrastive learning embeddings. During text generation, the model retrieves
relevant image, audio, and text examples to inform its completions. This grounds the output in additional context, improving relevance and
accuracy. Early results from Anthropic using this approach are incredible. On difficult prompts like imagining a day at the beach,
models augmented with image retrieval produce vastly more coherent, grounded, and specific outputs: Generated without MM-RAG : I imagine a
day at the beach would be very relaxing. I would sit in the sun and listen to the waves crash
along the shore. Maybe I would go for a swim or build a sandcastle. It would be nice to get
away from normal life for a while and enjoy the peaceful atmosphere. Generated with MM-RAG: I imagine a day at
the beach filled with golden sandy shores and the rhythmic crash of bright blue waves lapping gently at the coastline.
My toes would sink into smooth sand as I breathed in the fresh and briny sea air under a bright
sky, dotted with puffy clouds. I’d love to go snorkeling and glimpse the colorful fish swimming below or just nap
on a towel, lulled into a rest by the glittering water and crying seagulls swooping overhead. Maybe later I could
crack open a coconut or build an elaborate sandcastle with bridges and moats before taking a long walk at sunset,
watching the glowing orange disk sink below the horizon. The MM-RAG example conveys so much more specificity and flair thanks
to grounding from relevant imagery. Researchers have found similar benefits across many types of generative tasks from summarization to dialogue
modeling and more. This approach holds incredible promise for taking language AI to the next level. But we need scalable
ways to deliver both multimodal retrieval and foundation models in production systems. Next let’s explore how vector databases make this
possible. Building Multimodal Production Systems with Vector Databases The techniques we’ve explored offer game-changing capabilities — multimodal search and retrieval,
MM-RAG grounded language generation, and more. But delivering these systems at scale in production comes with challenges around efficiency, cost,
infrastructure complexity and more. This is where purpose-built vector databases like Weaviate come in. By efficiently indexing and searching vector
spaces, these databases unlock user-scale deployment of contrastive learning and neural representation models. For example, to enable super-fast multimodal search
and retrieval, we can use the following stack: Contrastively trained multimodal models like CLIP encode data points like images, text,
audio etc. into a shared vector space Vector database like Weaviate ingests these embeddings Database performs efficient vector similarity search
even with billions of embeddings User queries search vector database to instantly retrieve relevant results across modalities With specialized data
structures and algorithms tailored to vector spaces, this stack powers blazing fast multi-modal search even with huge data volumes. We
can use the same foundation to build MM-RAG production systems at scale: Ingest large foundation language model Allow model to
interface with vector database storing billions of multimodal embeddings During generation, efficiently retrieve relevant context to inform text completions Return
grounded, relevant output to users Continue to improve models through feedback loops By leveraging the combined powers of foundation models,
contrastive representation learning, and vector databases — this systems moves the promises of MM-RAG from research into real-world practice. The
Future of Multimodal AI This post has highlighted fascinating innovations like contrastive learning for multimodal representations, any-to-any search across modalities,
and MM-RAG grounded generation. Together, these techniques are expanding what AI can perceive, conceive and achieve. As methods mature further,
we’ll see multimodal AI grow more pervasive — from recommender systems like Meta understanding users rich interests, to virtual assistants
like GPT-4 and others answering questions more accurately. Media & e-commerce will allow fine-grained exploration of catalogs with any-to-any search.
Generative applications will produce writing, imagery, animation, synthesis and dialogue that maintains strong coherence and grounding. Making these futures reality
requires that we make multimodal AI scalable and deployable. Purpose-built vector databases unlock this critical step, enabling the incredible capabilities
discussed in production environments. I hope you’ve enjoyed this tour through the growing world of multimodal intelligence and learning how
databases are powering real-world applications leveraging techniques from contrastive learning to MM-RAG! Exciting times are ahead as multimodal and generative
AI continue rapid development.
