Retrieval-Augmented Generation(RAG) using open source LLMs. Sign up Sign in Sign up Sign in Kartheek Yakkala Follow -- Listen Share
Here, we are going to see how to use open source LLMs to develop an RAG chatbot. RAGs offers the
main limitation that usually LLMs have, i.e., their inability to access information beyond their training data cut-off point. They offer
dynamic solution by connecting LLMs with real-time data. They are usually used for document-question answering where user wants model to
answer from their data which might change from time to time, or come from an up-to-date data that was not
part of training. But they are not lii One might think of RAG as fine tuning, but both of them
are different. Fine tuning is the process of adapting a pre-trained model to certain dataset which updates few weights of
model to have the knowledge on new dataset as well. It still is static and doesn’t have knowledge on real-time
data. In contrast RAGs are dynamic and don’t update their weights based on new data. RAGs pull relevant information from
the new data and add them to the context that is provided to the model, such as prompts. RAG provides
a number of key benefits: 1. LLMs as reasoning engines 2. Reduce hallucinations 3. Domain-specific contextualisation 4. Efficiency and cost-effectiveness
RAGs can be built using mostly any LLM, such OpenAI’s GPT-3, GPT-4 and other open source models as well. In
this tutorial, we are going to see how to build RAG using open source LLMs. I’m using meta-llama/Llama-2-7b-chat-hf as my
LLM, it requires HuggingFace authentication. It is open source but requires HuggingFace token ( which is free of cost :)
). To start with lets install required libraries, in this tutorial we are using llamaindex as our framework As discussed
earlier the open source model we’re using requires Hugging face authentication Import required libraries We are using quantisation here, we
will not be able to load the whole meta-llama/Llama-2-7b-chat-hf in the free tier of colab or if your resources are
not enough. You can skip this step if you have enough resources. Converting messages to prompt. prompting is the fundamental
input that enables LLM application to be expressive. We are using below prompt template to enable LLM to work as
chatbot/assistant. Load LLM. There are a lot ways to load huggingface llms, here we are using local Hugging Face model
cache by downloading the model, and actually runs the model on our local machine’s hardware. We can also use the
HuggingFaceInferenceAPI which uses the same model, but does not save model in our cache it runs remotely on huggingface’s servers
and is accessible to us via API. More on this can be found here . Configure service context, it is
a bundle of configurations that are required by LLM application during indexing / querying stage. you can find more about
it here Load data into the index. Indexing documents is the key component in building an RAG application. you can
find more about VectorStoreIndex in llamaindex here Querying the model. Finally we have built the application, we have to query
against model. we use Query engine is a generic interface that allows you to ask question over your data. for
more information on RAGs please check this blog https://www.pinecone.io/learn/retrieval-augmented-generation/ For code, please check my github repo: https://github.com/kartheekyakkala/Chat-with-your-docs I have used
below documents as my source for learning and this tutorial. The Big Book of MLOps: Second Edition https://www.databricks.com/resources/ebook/the-big-book-of-mlops?scid=7018Y000001Fi1CQAS&utm_medium=paid+search&utm_source=google&utm_campaign=17107065832&utm_adgroup=145252855846&utm_content=ebook&utm_offer=the-big-book-of-mlops&utm_ad=678157791283&utm_term=databricks%20the%20big%20book%20of%20mlops&gad_source=1&gclid=Cj0KCQiAy9msBhD0ARIsANbk0A_KVrV7BLOnbXZ-gXCYcYt4IigRLrHN5v4kNg39_vJgVGYoD3qrmgQaAoiwEALw_wcB llamaindex documentation
docs.llamaindex.ai https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2 -- -- Software Engineer Help Status About Careers Press Blog Privacy Terms Text to speech Teams Retrieval-Augmented Generation(RAG)
using open source LLMs. Kartheek Yakkala · Follow 4 min read · Jan 4, 2024 -- Listen Share Here, we
are going to see how to use open source LLMs to develop an RAG chatbot. RAGs offers the main limitation
that usually LLMs have, i.e., their inability to access information beyond their training data cut-off point. They offer dynamic solution
by connecting LLMs with real-time data. They are usually used for document-question answering where user wants model to answer from
their data which might change from time to time, or come from an up-to-date data that was not part of
training. But they are not lii One might think of RAG as fine tuning, but both of them are different.
Fine tuning is the process of adapting a pre-trained model to certain dataset which updates few weights of model to
have the knowledge on new dataset as well. It still is static and doesn’t have knowledge on real-time data. In
contrast RAGs are dynamic and don’t update their weights based on new data. RAGs pull relevant information from the new
data and add them to the context that is provided to the model, such as prompts. Retrieval-Augmented Generation(RAG) workflow RAG
provides a number of key benefits: 1. LLMs as reasoning engines 2. Reduce hallucinations 3. Domain-specific contextualisation 4. Efficiency and
cost-effectiveness RAGs can be built using mostly any LLM, such OpenAI’s GPT-3, GPT-4 and other open source models as well.
In this tutorial, we are going to see how to build RAG using open source LLMs. I’m using meta-llama/Llama-2-7b-chat-hf as
my LLM, it requires HuggingFace authentication. It is open source but requires HuggingFace token ( which is free of cost
:) ). To start with lets install required libraries, in this tutorial we are using llamaindex as our framework pip
install llama-index transformers accelerate bitsandbytes pypdf As discussed earlier the open source model we’re using requires Hugging face authentication huggingface-cli
login Import required libraries from llama_index import VectorStoreIndex, SimpleDirectoryReader import torch from transformers import BitsAndBytesConfig from llama_index.prompts import PromptTemplate from
llama_index.llms import HuggingFaceLLM from llama_index import ServiceContext from llama_index.response.notebook_utils import display_response We are using quantisation here, we will not be
able to load the whole meta-llama/Llama-2-7b-chat-hf in the free tier of colab or if your resources are not enough. You
can skip this step if you have enough resources. quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, ) Converting messages to
prompt. prompting is the fundamental input that enables LLM application to be expressive. We are using below prompt template to
enable LLM to work as chatbot/assistant. def messages_to_prompt(messages): prompt = "" for message in messages: if message.role == 'system': prompt
+= f"<|system|>\n{message.content}\n" elif message.role == 'user': prompt += f"<|user|>\n{message.content}\n" elif message.role == 'assistant': prompt += f"<|assistant|>\n{message.content}\n" # ensure we start
with a system prompt, insert blank if needed if not prompt.startswith("<|system|>\n"): prompt = "<|system|>\n\n" + prompt # add final assistant
prompt prompt = prompt + "<|assistant|>\n" return prompt Load LLM. There are a lot ways to load huggingface llms, here
we are using local Hugging Face model cache by downloading the model, and actually runs the model on our local
machine’s hardware. We can also use the HuggingFaceInferenceAPI which uses the same model, but does not save model in our
cache it runs remotely on huggingface’s servers and is accessible to us via API. More on this can be found
here . llm = HuggingFaceLLM( model_name="meta-llama/Llama-2-7b-chat-hf", tokenizer_name="meta-llama/Llama-2-7b-chat-hf", query_wrapper_prompt=PromptTemplate("<|system|>\n\n<|user|>\n{query_str}\n<|assistant|>\n"), context_window=3900, max_new_tokens=256, model_kwargs={"quantization_config": quantization_config}, # tokenizer_kwargs={}, generate_kwargs={"temperature": 0.3, "top_k": 50, "top_p": 0.95},
messages_to_prompt=messages_to_prompt, device_map="auto", ) Configure service context, it is a bundle of configurations that are required by LLM application during indexing
/ querying stage. you can find more about it here service_context = ServiceContext.from_defaults(llm=llm, embed_model="local:BAAI/bge-small-en-v1.5") Load data into the index. Indexing
documents is the key component in building an RAG application. you can find more about VectorStoreIndex in llamaindex here vector_index
= VectorStoreIndex.from_documents(documents, service_context=service_context) Querying the model. Finally we have built the application, we have to query against model. we use
Query engine is a generic interface that allows you to ask question over your data. query_engine = vector_index.as_query_engine(response_mode="compact") response =
query_engine.query("<give your question>") # give your question here display_response(response) for more information on RAGs please check this blog https://www.pinecone.io/learn/retrieval-augmented-generation/ For
code, please check my github repo: https://github.com/kartheekyakkala/Chat-with-your-docs I have used below documents as my source for learning and this tutorial.
The Big Book of MLOps: Second Edition https://www.databricks.com/resources/ebook/the-big-book-of-mlops?scid=7018Y000001Fi1CQAS&utm_medium=paid+search&utm_source=google&utm_campaign=17107065832&utm_adgroup=145252855846&utm_content=ebook&utm_offer=the-big-book-of-mlops&utm_ad=678157791283&utm_term=databricks%20the%20big%20book%20of%20mlops&gad_source=1&gclid=Cj0KCQiAy9msBhD0ARIsANbk0A_KVrV7BLOnbXZ-gXCYcYt4IigRLrHN5v4kNg39_vJgVGYoD3qrmgQaAoiwEALw_wcB llamaindex documentation LlamaIndex 🦙 0.9.25.post1 LlamaIndex is a data framework for LLM-based
applications to ingest, structure, and access private or domain-specific… docs.llamaindex.ai https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2 Retrieval-Augmented Generation(RAG) using open source LLMs. Kartheek Yakkala ·
Follow 4 min read · Jan 4, 2024 -- Listen Share Here, we are going to see how to use
open source LLMs to develop an RAG chatbot. RAGs offers the main limitation that usually LLMs have, i.e., their inability
to access information beyond their training data cut-off point. They offer dynamic solution by connecting LLMs with real-time data. They
are usually used for document-question answering where user wants model to answer from their data which might change from time
to time, or come from an up-to-date data that was not part of training. But they are not lii One
might think of RAG as fine tuning, but both of them are different. Fine tuning is the process of adapting
a pre-trained model to certain dataset which updates few weights of model to have the knowledge on new dataset as
well. It still is static and doesn’t have knowledge on real-time data. In contrast RAGs are dynamic and don’t update
their weights based on new data. RAGs pull relevant information from the new data and add them to the context
that is provided to the model, such as prompts. Retrieval-Augmented Generation(RAG) workflow RAG provides a number of key benefits: 1.
LLMs as reasoning engines 2. Reduce hallucinations 3. Domain-specific contextualisation 4. Efficiency and cost-effectiveness RAGs can be built using mostly
any LLM, such OpenAI’s GPT-3, GPT-4 and other open source models as well. In this tutorial, we are going to
see how to build RAG using open source LLMs. I’m using meta-llama/Llama-2-7b-chat-hf as my LLM, it requires HuggingFace authentication. It
is open source but requires HuggingFace token ( which is free of cost :) ). To start with lets install
required libraries, in this tutorial we are using llamaindex as our framework pip install llama-index transformers accelerate bitsandbytes pypdf As
discussed earlier the open source model we’re using requires Hugging face authentication huggingface-cli login Import required libraries from llama_index import
VectorStoreIndex, SimpleDirectoryReader import torch from transformers import BitsAndBytesConfig from llama_index.prompts import PromptTemplate from llama_index.llms import HuggingFaceLLM from llama_index import ServiceContext
from llama_index.response.notebook_utils import display_response We are using quantisation here, we will not be able to load the whole meta-llama/Llama-2-7b-chat-hf in
the free tier of colab or if your resources are not enough. You can skip this step if you have
enough resources. quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, ) Converting messages to prompt. prompting is the fundamental input that
enables LLM application to be expressive. We are using below prompt template to enable LLM to work as chatbot/assistant. def
messages_to_prompt(messages): prompt = "" for message in messages: if message.role == 'system': prompt += f"<|system|>\n{message.content}\n" elif message.role == 'user': prompt
+= f"<|user|>\n{message.content}\n" elif message.role == 'assistant': prompt += f"<|assistant|>\n{message.content}\n" # ensure we start with a system prompt, insert blank if
needed if not prompt.startswith("<|system|>\n"): prompt = "<|system|>\n\n" + prompt # add final assistant prompt prompt = prompt + "<|assistant|>\n" return
prompt Load LLM. There are a lot ways to load huggingface llms, here we are using local Hugging Face model
cache by downloading the model, and actually runs the model on our local machine’s hardware. We can also use the
HuggingFaceInferenceAPI which uses the same model, but does not save model in our cache it runs remotely on huggingface’s servers
and is accessible to us via API. More on this can be found here . llm = HuggingFaceLLM( model_name="meta-llama/Llama-2-7b-chat-hf", tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
query_wrapper_prompt=PromptTemplate("<|system|>\n\n<|user|>\n{query_str}\n<|assistant|>\n"), context_window=3900, max_new_tokens=256, model_kwargs={"quantization_config": quantization_config}, # tokenizer_kwargs={}, generate_kwargs={"temperature": 0.3, "top_k": 50, "top_p": 0.95}, messages_to_prompt=messages_to_prompt, device_map="auto", ) Configure service context, it
is a bundle of configurations that are required by LLM application during indexing / querying stage. you can find more
about it here service_context = ServiceContext.from_defaults(llm=llm, embed_model="local:BAAI/bge-small-en-v1.5") Load data into the index. Indexing documents is the key component in building
an RAG application. you can find more about VectorStoreIndex in llamaindex here vector_index = VectorStoreIndex.from_documents(documents, service_context=service_context) Querying the model. Finally
we have built the application, we have to query against model. we use Query engine is a generic interface that
allows you to ask question over your data. query_engine = vector_index.as_query_engine(response_mode="compact") response = query_engine.query("<give your question>") # give your question
here display_response(response) for more information on RAGs please check this blog https://www.pinecone.io/learn/retrieval-augmented-generation/ For code, please check my github repo: https://github.com/kartheekyakkala/Chat-with-your-docs
I have used below documents as my source for learning and this tutorial. The Big Book of MLOps: Second Edition
https://www.databricks.com/resources/ebook/the-big-book-of-mlops?scid=7018Y000001Fi1CQAS&utm_medium=paid+search&utm_source=google&utm_campaign=17107065832&utm_adgroup=145252855846&utm_content=ebook&utm_offer=the-big-book-of-mlops&utm_ad=678157791283&utm_term=databricks%20the%20big%20book%20of%20mlops&gad_source=1&gclid=Cj0KCQiAy9msBhD0ARIsANbk0A_KVrV7BLOnbXZ-gXCYcYt4IigRLrHN5v4kNg39_vJgVGYoD3qrmgQaAoiwEALw_wcB llamaindex documentation LlamaIndex 🦙 0.9.25.post1 LlamaIndex is a data framework for LLM-based applications to ingest, structure, and access private
or domain-specific… docs.llamaindex.ai https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2
