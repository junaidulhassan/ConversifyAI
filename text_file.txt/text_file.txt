Introduction LangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage of the
LLM application lifecycle: Concretely, the framework consists of the following open-source libraries: These docs focus on the Python LangChain library.
Head here for docs on the JavaScript LangChain library. If you're looking to build something specific or are more of
a hands-on learner, check out our tutorials section . This is the best place to get started. These are the
best ones to get started with: Explore the full list of LangChain tutorials here , and check out other LangGraph
tutorials here . Here you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions. These how-to guides don‚Äôt cover
topics in depth ‚Äì you‚Äôll find that material in the Tutorials and the API Reference . However, these guides will
help you quickly accomplish common tasks. Check out LangGraph-specific how-tos here . Introductions to all the key parts of LangChain
you‚Äôll need to know! Here you'll find high level explanations of all LangChain concepts. For a deeper dive into LangGraph
concepts, check out this page . Head to the reference section for full documentation of all classes and methods in
the LangChain Python packages. Trace and evaluate your language model applications and intelligent agents to help you move from prototype
to production. Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. Read up
on our Security best practices to make sure you're developing safely with LangChain. LangChain is part of a rich ecosystem
of tools that integrate with our framework and build on top of it. Check out our growing list of integrations
. Check out the developer's guide for guidelines on contributing and help getting your dev environment set up. Introduction On
this page Introduction LangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage
of the LLM application lifecycle: Development : Build your applications using LangChain's open-source building blocks , components , and third-party
integrations . Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support. Productionization : Use LangSmith to inspect,
monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence. Deployment : Turn your LangGraph
applications into production-ready APIs and Assistants with LangGraph Cloud . Concretely, the framework consists of the following open-source libraries: langchain-core
: Base abstractions and LangChain Expression Language. langchain-community : Third party integrations. Partner packages (e.g. langchain-openai , langchain-anthropic , etc.):
Some integrations have been further split into their own lightweight packages that only depend on langchain-core . langchain : Chains,
agents, and retrieval strategies that make up an application's cognitive architecture. LangGraph : Build robust and stateful multi-actor applications with
LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without
it. LangServe : Deploy LangChain chains as REST APIs. LangSmith : A developer platform that lets you debug, test, evaluate,
and monitor LLM applications. note These docs focus on the Python LangChain library. Head here for docs on the JavaScript
LangChain library. Tutorials ‚Äã If you're looking to build something specific or are more of a hands-on learner, check out
our tutorials section . This is the best place to get started. These are the best ones to get started
with: Build a Simple LLM Application Build a Chatbot Build an Agent Introduction to LangGraph Explore the full list of
LangChain tutorials here , and check out other LangGraph tutorials here . How-to guides ‚Äã Here you‚Äôll find short answers
to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions. These how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material
in the Tutorials and the API Reference . However, these guides will help you quickly accomplish common tasks. Check out
LangGraph-specific how-tos here . Conceptual guide ‚Äã Introductions to all the key parts of LangChain you‚Äôll need to know! Here
you'll find high level explanations of all LangChain concepts. For a deeper dive into LangGraph concepts, check out this page
. API reference ‚Äã Head to the reference section for full documentation of all classes and methods in the LangChain
Python packages. Ecosystem ‚Äã ü¶úüõ†Ô∏è LangSmith ‚Äã Trace and evaluate your language model applications and intelligent agents to help you
move from prototype to production. ü¶úüï∏Ô∏è LangGraph ‚Äã Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can
be used without it. Additional resources ‚Äã Security ‚Äã Read up on our Security best practices to make sure you're
developing safely with LangChain. Integrations ‚Äã LangChain is part of a rich ecosystem of tools that integrate with our framework
and build on top of it. Check out our growing list of integrations . Contributing ‚Äã Check out the developer's
guide for guidelines on contributing and help getting your dev environment set up. Edit this page People Contributing Templates Cookbooks
3rd party tutorials YouTube arXiv v0.2 v0.1 LangSmith LangSmith Docs Templates GitHub Templates Hub LangChain Hub JS/TS Docs Introduction Tutorials
Build a Question Answering application over a Graph Database Tutorials Build a Simple LLM Application with LCEL Build a Query
Analysis System Build a Chatbot Conversational RAG Build an Extraction Chain Build an Agent Tagging data_generation Build a Local RAG
Application Build a PDF ingestion and Question/Answering system Build a Retrieval Augmented Generation (RAG) App Vector stores and retrievers Build
a Question/Answering system over SQL data Summarize Text Build a Question Answering application over a Graph Database Tutorials Build a
Simple LLM Application with LCEL Build a Query Analysis System Build a Chatbot Conversational RAG Build an Extraction Chain Build
an Agent Tagging data_generation Build a Local RAG Application Build a PDF ingestion and Question/Answering system Build a Retrieval Augmented
Generation (RAG) App Vector stores and retrievers Build a Question/Answering system over SQL data Summarize Text How-to guides How-to guides
How to use tools in a chain How to use a vectorstore as a retriever How to add memory to
chatbots How to use example selectors How to map values to a graph database How to add a semantic layer
over graph database How to invoke runnables in parallel How to stream chat model responses How to add default invocation
args to a Runnable How to add retrieval to chatbots How to use few shot examples in chat models How
to do tool/function calling How to best prompt for Graph-RAG How to install LangChain packages How to add examples to
the prompt for query analysis How to use few shot examples How to run custom functions How to use output
parsers to parse an LLM response into structured format How to handle cases where no queries are generated How to
route between sub-chains How to return structured data from a model How to use toolkits How to add ad-hoc tool
calling capability to LLMs and Chat Models Build an Agent with AgentExecutor (Legacy) How to construct knowledge graphs How to
partially format prompt templates How to handle multiple queries when doing query analysis How to use built-in tools and toolkits
How to pass through arguments from one step to the next How to compose prompts together How to handle multiple
retrievers when doing query analysis How to add values to a chain's state How to construct filters for query analysis
How to configure runtime chain internals How deal with high cardinality categoricals when doing query analysis Custom Document Loader How
to split by HTML header How to split by HTML sections How to use the MultiQueryRetriever How to add scores
to retriever results Caching How to use callbacks in async environments How to attach callbacks to a runnable How to
propagate callbacks constructor How to dispatch custom callback events How to pass callbacks in at runtime How to split by
character How to cache chat model responses How to handle rate limits How to init any model in one line
How to track token usage in ChatModels How to add tools to chatbots How to split code How to do
retrieval with contextual compression How to convert Runnables as Tools How to create custom callback handlers How to create a
custom chat model class How to create a custom LLM class Custom Retriever How to create tools How to debug
your LLM apps How to load CSVs How to load documents from a directory How to load HTML How to
load JSON How to load Markdown How to load Microsoft Office files How to load PDFs How to create a
dynamic (self-constructing) chain Text embedding models How to combine results from multiple retrievers How to select examples by length How
to select examples by maximal marginal relevance (MMR) How to select examples by n-gram overlap How to select examples by
similarity How to use reference examples when doing extraction How to handle long text when doing extraction How to use
prompting alone (no tool calling) to do extraction How to add fallbacks to a runnable How to filter messages Hybrid
Search How to use the LangChain indexing API How to inspect runnables LangChain Expression Language Cheatsheet How to cache LLM
responses How to track token usage for LLMs Run models locally How to get log probabilities How to reorder retrieved
results to mitigate the "lost in the middle" effect How to split Markdown by Headers How to merge consecutive messages
of the same type How to add message history How to migrate from legacy LangChain agents to LangGraph How to
retrieve using multiple vectors per document How to pass multimodal data directly to models How to use multimodal prompts How
to create a custom Output Parser How to use the output-fixing parser How to parse JSON output How to retry
when a parsing error occurs How to parse XML output How to parse YAML output How to use the Parent
Document Retriever How to use LangChain with different Pydantic versions How to add chat history How to get a RAG
application to add citations How to do per-user retrieval How to get your RAG application to return sources How to
stream results from your RAG application How to split JSON data How to recursively split text by characters Response metadata
How to pass runtime secrets to runnables How to do "self-querying" retrieval How to split text based on semantic similarity
How to chain runnables How to save and load LangChain objects How to split text by tokens How to do
question answering over CSVs How to deal with large databases when doing SQL question-answering How to better prompt when doing
SQL question-answering How to do query validation as part of SQL question-answering How to stream runnables How to stream responses
from an LLM How to use a time-weighted vector store retriever How to return artifacts from a tool How to
use chat models to call tools How to disable parallel tool calling How to force models to call a tool
How to access the RunnableConfig from a tool How to pass tool outputs to chat models How to pass run
time values to tools How to stream events from a tool How to stream tool calls How to convert tools
to OpenAI Functions How to handle tool errors How to use few-shot prompting with tool calling How to add a
human-in-the-loop for tools How to bind model-specific tools How to trim messages How to create and query vector stores How-to
guides How to use tools in a chain How to use a vectorstore as a retriever How to add memory
to chatbots How to use example selectors How to map values to a graph database How to add a semantic
layer over graph database How to invoke runnables in parallel How to stream chat model responses How to add default
invocation args to a Runnable How to add retrieval to chatbots How to use few shot examples in chat models
How to do tool/function calling How to best prompt for Graph-RAG How to install LangChain packages How to add examples
to the prompt for query analysis How to use few shot examples How to run custom functions How to use
output parsers to parse an LLM response into structured format How to handle cases where no queries are generated How
to route between sub-chains How to return structured data from a model How to use toolkits How to add ad-hoc
tool calling capability to LLMs and Chat Models Build an Agent with AgentExecutor (Legacy) How to construct knowledge graphs How
to partially format prompt templates How to handle multiple queries when doing query analysis How to use built-in tools and
toolkits How to pass through arguments from one step to the next How to compose prompts together How to handle
multiple retrievers when doing query analysis How to add values to a chain's state How to construct filters for query
analysis How to configure runtime chain internals How deal with high cardinality categoricals when doing query analysis Custom Document Loader
How to split by HTML header How to split by HTML sections How to use the MultiQueryRetriever How to add
scores to retriever results Caching How to use callbacks in async environments How to attach callbacks to a runnable How
to propagate callbacks constructor How to dispatch custom callback events How to pass callbacks in at runtime How to split
by character How to cache chat model responses How to handle rate limits How to init any model in one
line How to track token usage in ChatModels How to add tools to chatbots How to split code How to
do retrieval with contextual compression How to convert Runnables as Tools How to create custom callback handlers How to create
a custom chat model class How to create a custom LLM class Custom Retriever How to create tools How to
debug your LLM apps How to load CSVs How to load documents from a directory How to load HTML How
to load JSON How to load Markdown How to load Microsoft Office files How to load PDFs How to create
a dynamic (self-constructing) chain Text embedding models How to combine results from multiple retrievers How to select examples by length
How to select examples by maximal marginal relevance (MMR) How to select examples by n-gram overlap How to select examples
by similarity How to use reference examples when doing extraction How to handle long text when doing extraction How to
use prompting alone (no tool calling) to do extraction How to add fallbacks to a runnable How to filter messages
Hybrid Search How to use the LangChain indexing API How to inspect runnables LangChain Expression Language Cheatsheet How to cache
LLM responses How to track token usage for LLMs Run models locally How to get log probabilities How to reorder
retrieved results to mitigate the "lost in the middle" effect How to split Markdown by Headers How to merge consecutive
messages of the same type How to add message history How to migrate from legacy LangChain agents to LangGraph How
to retrieve using multiple vectors per document How to pass multimodal data directly to models How to use multimodal prompts
How to create a custom Output Parser How to use the output-fixing parser How to parse JSON output How to
retry when a parsing error occurs How to parse XML output How to parse YAML output How to use the
Parent Document Retriever How to use LangChain with different Pydantic versions How to add chat history How to get a
RAG application to add citations How to do per-user retrieval How to get your RAG application to return sources How
to stream results from your RAG application How to split JSON data How to recursively split text by characters Response
metadata How to pass runtime secrets to runnables How to do "self-querying" retrieval How to split text based on semantic
similarity How to chain runnables How to save and load LangChain objects How to split text by tokens How to
do question answering over CSVs How to deal with large databases when doing SQL question-answering How to better prompt when
doing SQL question-answering How to do query validation as part of SQL question-answering How to stream runnables How to stream
responses from an LLM How to use a time-weighted vector store retriever How to return artifacts from a tool How
to use chat models to call tools How to disable parallel tool calling How to force models to call a
tool How to access the RunnableConfig from a tool How to pass tool outputs to chat models How to pass
run time values to tools How to stream events from a tool How to stream tool calls How to convert
tools to OpenAI Functions How to handle tool errors How to use few-shot prompting with tool calling How to add
a human-in-the-loop for tools How to bind model-specific tools How to trim messages How to create and query vector stores
Conceptual guide Ecosystem ü¶úüõ†Ô∏è LangSmith ü¶úüï∏Ô∏è LangGraph ü¶úüõ†Ô∏è LangSmith ü¶úüï∏Ô∏è LangGraph Versions Overview of v0.2 Release policy Pydantic compatibility Migrating
to v0.2 Migrating to LangChain v0.2 astream_events v2 Changes Migrating from v0.0 chains How to migrate from v0.0 chains Migrating
from ConversationalChain Migrating from ConversationalRetrievalChain Migrating from LLMChain Migrating from LLMRouterChain Migrating from MapReduceDocumentsChain Migrating from MapRerankDocumentsChain Migrating from MultiPromptChain
Migrating from RefineDocumentsChain Migrating from RetrievalQA Migrating from StuffDocumentsChain Overview of v0.2 Release policy Pydantic compatibility Migrating to v0.2 Migrating
to LangChain v0.2 astream_events v2 Changes Migrating to LangChain v0.2 astream_events v2 Changes Migrating from v0.0 chains How to migrate
from v0.0 chains Migrating from ConversationalChain Migrating from ConversationalRetrievalChain Migrating from LLMChain Migrating from LLMRouterChain Migrating from MapReduceDocumentsChain Migrating from
MapRerankDocumentsChain Migrating from MultiPromptChain Migrating from RefineDocumentsChain Migrating from RetrievalQA Migrating from StuffDocumentsChain How to migrate from v0.0 chains Migrating
from ConversationalChain Migrating from ConversationalRetrievalChain Migrating from LLMChain Migrating from LLMRouterChain Migrating from MapReduceDocumentsChain Migrating from MapRerankDocumentsChain Migrating from MultiPromptChain
Migrating from RefineDocumentsChain Migrating from RetrievalQA Migrating from StuffDocumentsChain Security Introduction Development : Build your applications using LangChain's open-source building
blocks , components , and third-party integrations . Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.
Productionization : Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with
confidence. Deployment : Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud . langchain-core : Base abstractions
and LangChain Expression Language. langchain-community : Third party integrations. Partner packages (e.g. langchain-openai , langchain-anthropic , etc.): Some integrations have
been further split into their own lightweight packages that only depend on langchain-core . Partner packages (e.g. langchain-openai , langchain-anthropic
, etc.): Some integrations have been further split into their own lightweight packages that only depend on langchain-core . langchain
: Chains, agents, and retrieval strategies that make up an application's cognitive architecture. LangGraph : Build robust and stateful multi-actor
applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be
used without it. LangServe : Deploy LangChain chains as REST APIs. LangSmith : A developer platform that lets you debug,
test, evaluate, and monitor LLM applications. Build a Simple LLM Application Build a Chatbot Build an Agent Introduction to LangGraph
Tutorials How-to guides Conceptual guide API reference Ecosystem ü¶úüõ†Ô∏è LangSmith ü¶úüï∏Ô∏è LangGraph ü¶úüõ†Ô∏è LangSmith ü¶úüï∏Ô∏è LangGraph Additional resources Security Integrations
Contributing Security Integrations Contributing Twitter Organization Python JS/TS Homepage Blog YouTube
