{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrap import Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://medium.com/@bijit211987/multimodal-retrieval-augmented-generation-mm-rag-2e8f6dc59f11\"\n",
    "scrp = Scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrp.scrape_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAG_QnA import RAG_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 16:18:45.697534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-28 16:18:45.883812: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-28 16:18:45.934653: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-28 16:18:49.128005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/junaid-ul-hassan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "rag_model = RAG_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 50 is greater than number of elements in index 32, updating n_results = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. MM-RAG (Multimodal Retrieval Augmented Generation) augments language generation models with multimodal retrieval capabilities.\n",
      "   2. This technique combines a language model like GPT-3 with a multimodal retriever using contrastive learning embeddings.\n",
      "   3. The combination of language model and multimodal retriever helps ground the output in additional context, improving relevance and accuracy.\n",
      "   4. In essence, MM-RAG allows for context-rich language generation that takes into account various modes of data, including images, text, audio, and video.\n",
      "   5. As a result, generated responses are more coherent, grounded, and specific compared to language models without MM-RAG.\n",
      "   6. MM-RAG grounded language generation, improved search and retrieval, and more.\n",
      "   7. To enable super-fast multimodal search and retrieval, specialized databases like Weaviate play a crucial role, providing efficient vector similarity search even with billions of embeddings.\n",
      "   8. Thanks for asking!\n",
      "\n",
      "References:\n",
      "1. Brown et al., Language Models Are Few-Shot Learners. 2020\n",
      "2. Guo et al., Multimodal Embeddings via Cross-Modal Contrastive Learning. 2020\n",
      "3. Dhariwal, Kannan, Wallace, Wu, Shu, Ziegler, Weston, Li, Amodei, Sutskever, & Chorowski, MM-RAG: Augmenting Generation with Multimodal Retrieval. 2021\n",
      "4. Whitehead et al., Weaviate: A scalable vector database for machine learning and artificial intelligence. 2021.</s>\n"
     ]
    }
   ],
   "source": [
    "print(rag_model.generateResponse(prompt=\"Why we shouuld use RAG?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='audio, and text examples to inform its completions. This grounds the output in additional context, improving relevance and accuracy. Early\\nresults from Anthropic using this approach are incredible. On difficult prompts like imagining a day at the beach, models augmented\\nwith image retrieval produce vastly more coherent, grounded, and specific outputs: Generated without MM-RAG : I imagine a day at\\nthe beach would be very relaxing. I would sit in the sun and listen to the waves crash along the\\nshore. Maybe I would go for a swim or build a sandcastle. It would be nice to get away from\\nnormal life for a while and enjoy the peaceful atmosphere. Generated with MM-RAG: I imagine a day at the beach\\nfilled with golden sandy shores and the rhythmic crash of bright blue waves lapping gently at the coastline. My toes\\nwould sink into smooth sand as I breathed in the fresh and briny sea air under a bright sky, dotted', metadata={'source': '/media/junaid-ul-hassan/248ac48e-ccd4-4707-a28b-33cb7a46e6dc/LLMs Projects/Web_pilot/text_file.txt/text_file.txt'}),\n",
       " Document(page_content='model retrieves relevant image, audio, and text examples to inform its completions. This grounds the output in additional context, improving\\nrelevance and accuracy. Early results from Anthropic using this approach are incredible. On difficult prompts like imagining a day at\\nthe beach, models augmented with image retrieval produce vastly more coherent, grounded, and specific outputs: Generated without MM-RAG : I\\nimagine a day at the beach would be very relaxing. I would sit in the sun and listen to the\\nwaves crash along the shore. Maybe I would go for a swim or build a sandcastle. It would be nice\\nto get away from normal life for a while and enjoy the peaceful atmosphere. Generated with MM-RAG: I imagine a\\nday at the beach filled with golden sandy shores and the rhythmic crash of bright blue waves lapping gently at\\nthe coastline. My toes would sink into smooth sand as I breathed in the fresh and briny sea air under', metadata={'source': '/media/junaid-ul-hassan/248ac48e-ccd4-4707-a28b-33cb7a46e6dc/LLMs Projects/Web_pilot/text_file.txt/text_file.txt'}),\n",
       " Document(page_content='incredible capabilities discussed in production environments. I hope you’ve enjoyed this tour through the growing world of multimodal intelligence and\\nlearning how databases are powering real-world applications leveraging techniques from contrastive learning to MM-RAG! Exciting times are ahead as multimodal\\nand generative AI continue rapid development. Multimodal Retrieval Augmented Generation (MM-RAG) Bijit Ghosh · Follow 6 min read · Jan\\n21, 2024 -- Listen Share Introduction Multimodal machine learning is revolutionizing what AI systems can do. By understanding different modalities\\nlike images, audio, video, and text, these systems can solve problems that were previously intractable for machines. Let’s explore an\\nexciting development in this field — Multimodal Retrieval Augmented Generation (MM-RAG) — and how vector databases enable us to build', metadata={'source': '/media/junaid-ul-hassan/248ac48e-ccd4-4707-a28b-33cb7a46e6dc/LLMs Projects/Web_pilot/text_file.txt/text_file.txt'}),\n",
       " Document(page_content='Multimodal Retrieval Augmented Generation (MM-RAG) Introduction Contrastive Learning for Multimodal Representations Any-to-Any Search with Multimodal Embeddings MM-RAG: Augmenting Generation with\\nMultimodal Retrieval Building Multimodal Production Systems with Vector Databases The Future of Multimodal AI Sign up Sign in Sign up\\nSign in Bijit Ghosh Follow -- Listen Share Multimodal machine learning is revolutionizing what AI systems can do. By understanding\\ndifferent modalities like images, audio, video, and text, these systems can solve problems that were previously intractable for machines. Let’s\\nexplore an exciting development in this field — Multimodal Retrieval Augmented Generation (MM-RAG) — and how vector databases enable us\\nto build practical applications powered by multimodal embeddings for any-to-any search and retrieval. We’ll start by explaining contrastive learning, a', metadata={'source': '/media/junaid-ul-hassan/248ac48e-ccd4-4707-a28b-33cb7a46e6dc/LLMs Projects/Web_pilot/text_file.txt/text_file.txt'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"What is RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
